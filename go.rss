<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://azu.github.io/github-advisory-database-rss/go.rss</id>
    <title>Security Advisory for Go modules</title>
    <updated>2025-05-01T17:01:22.253Z</updated>
    <generator>github-advisory-database-rss</generator>
    <link rel="alternate" href="https://github.com/advisories?query=type%3Areviewed+ecosystem%3Ago"/>
    <subtitle>Security Advisory for Go modules on GitHub</subtitle>
    <rights>github-advisory-database-rss</rights>
    <category term="CRITICAL"/>
    <category term="HIGH"/>
    <category term="MODERATE"/>
    <category term="LOW"/>
    <entry>
        <title type="html"><![CDATA[[github.com/openfga/openfga] OpenFGA Authorization Bypass]]></title>
        <id>https://github.com/advisories/GHSA-w222-m46c-mgh6</id>
        <link href="https://github.com/advisories/GHSA-w222-m46c-mgh6"/>
        <updated>2025-05-01T13:30:20.000Z</updated>
        <content type="html"><![CDATA[<p>Overview
OpenFGA v1.8.10 or previous (Helm chart &lt;= openfga-0.2.28, docker &lt;= v.1.8.10) are vulnerable to authorization bypass when certain Check and ListObject calls are executed.</p>
<p>Am I Affected?
If you are using OpenFGA v1.8.10 or previous, specifically under the following conditions, you are affected by this authorization bypass vulnerability:</p>
<ul>
<li>Calling Check API or ListObjects with an <a href="https://openfga.dev/docs/concepts#what-is-an-authorization-model">authorization model</a> that has tuple cycle.</li>
<li><a href="https://github.com/openfga/openfga/blob/9b5974458b777707ed2a30ba6303699499e655ee/.config-schema.json#L528">Check query cache</a> is enabled, and</li>
<li>There are multiple check / list objects requests involving the tuple cycle within the check query TTL</li>
</ul>
<p>Fix
Upgrade to v1.8.11. This upgrade is backwards compatible.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/openfga/openfga/security/advisories/GHSA-w222-m46c-mgh6">https://github.com/openfga/openfga/security/advisories/GHSA-w222-m46c-mgh6</a></li>
<li><a href="https://github.com/openfga/openfga/commit/244302e7a8b979d66cc1874a3899cdff7d47862f">https://github.com/openfga/openfga/commit/244302e7a8b979d66cc1874a3899cdff7d47862f</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-46331">https://nvd.nist.gov/vuln/detail/CVE-2025-46331</a></li>
<li><a href="https://github.com/advisories/GHSA-w222-m46c-mgh6">https://github.com/advisories/GHSA-w222-m46c-mgh6</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-w222-m46c-mgh6</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2025-04-30T16:43:33.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[volcano.sh/volcano] Volcano Scheduler Denial of Service via Unbounded Response from Elastic Service/extender Plugin]]></title>
        <id>https://github.com/advisories/GHSA-hg79-fw4p-25p8</id>
        <link href="https://github.com/advisories/GHSA-hg79-fw4p-25p8"/>
        <updated>2025-05-01T13:30:15.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>This issue allows an attacker who has compromised either the Elastic service or the extender plugin to cause denial of service of the scheduler. This is a privilege escalation, because Volcano users may run their Elastic service and extender plugins in separate pods or nodes from the scheduler. In the Kubernetes security model, node isolation is a security boundary, and as such an attacker is able to cross that boundary in Volcano's case if they have compromised either the vulnerable services or the pod/node in which they are deployed.  The scheduler will become unavailable to other users and workloads in the cluster. The scheduler will either crash with an unrecoverable OOM panic or freeze while consuming excessive amounts of memory.</p>
<h3 id="workarounds">Workarounds</h3>
<p>No</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8">https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398">https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d">https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a">https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3">https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4">https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-32777">https://nvd.nist.gov/vuln/detail/CVE-2025-32777</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.10.2">https://github.com/volcano-sh/volcano/releases/tag/v1.10.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3">https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.2">https://github.com/volcano-sh/volcano/releases/tag/v1.11.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2">https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.9.1">https://github.com/volcano-sh/volcano/releases/tag/v1.9.1</a></li>
<li><a href="https://github.com/advisories/GHSA-hg79-fw4p-25p8">https://github.com/advisories/GHSA-hg79-fw4p-25p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hg79-fw4p-25p8</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-30T16:40:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[volcano.sh/volcano] Volcano Scheduler Denial of Service via Unbounded Response from Elastic Service/extender Plugin]]></title>
        <id>https://github.com/advisories/GHSA-hg79-fw4p-25p8</id>
        <link href="https://github.com/advisories/GHSA-hg79-fw4p-25p8"/>
        <updated>2025-05-01T13:30:15.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>This issue allows an attacker who has compromised either the Elastic service or the extender plugin to cause denial of service of the scheduler. This is a privilege escalation, because Volcano users may run their Elastic service and extender plugins in separate pods or nodes from the scheduler. In the Kubernetes security model, node isolation is a security boundary, and as such an attacker is able to cross that boundary in Volcano's case if they have compromised either the vulnerable services or the pod/node in which they are deployed.  The scheduler will become unavailable to other users and workloads in the cluster. The scheduler will either crash with an unrecoverable OOM panic or freeze while consuming excessive amounts of memory.</p>
<h3 id="workarounds">Workarounds</h3>
<p>No</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8">https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398">https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d">https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a">https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3">https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4">https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-32777">https://nvd.nist.gov/vuln/detail/CVE-2025-32777</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.10.2">https://github.com/volcano-sh/volcano/releases/tag/v1.10.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3">https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.2">https://github.com/volcano-sh/volcano/releases/tag/v1.11.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2">https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.9.1">https://github.com/volcano-sh/volcano/releases/tag/v1.9.1</a></li>
<li><a href="https://github.com/advisories/GHSA-hg79-fw4p-25p8">https://github.com/advisories/GHSA-hg79-fw4p-25p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hg79-fw4p-25p8</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-30T16:40:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[volcano.sh/volcano] Volcano Scheduler Denial of Service via Unbounded Response from Elastic Service/extender Plugin]]></title>
        <id>https://github.com/advisories/GHSA-hg79-fw4p-25p8</id>
        <link href="https://github.com/advisories/GHSA-hg79-fw4p-25p8"/>
        <updated>2025-05-01T13:30:15.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>This issue allows an attacker who has compromised either the Elastic service or the extender plugin to cause denial of service of the scheduler. This is a privilege escalation, because Volcano users may run their Elastic service and extender plugins in separate pods or nodes from the scheduler. In the Kubernetes security model, node isolation is a security boundary, and as such an attacker is able to cross that boundary in Volcano's case if they have compromised either the vulnerable services or the pod/node in which they are deployed.  The scheduler will become unavailable to other users and workloads in the cluster. The scheduler will either crash with an unrecoverable OOM panic or freeze while consuming excessive amounts of memory.</p>
<h3 id="workarounds">Workarounds</h3>
<p>No</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8">https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398">https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d">https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a">https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3">https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4">https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-32777">https://nvd.nist.gov/vuln/detail/CVE-2025-32777</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.10.2">https://github.com/volcano-sh/volcano/releases/tag/v1.10.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3">https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.2">https://github.com/volcano-sh/volcano/releases/tag/v1.11.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2">https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.9.1">https://github.com/volcano-sh/volcano/releases/tag/v1.9.1</a></li>
<li><a href="https://github.com/advisories/GHSA-hg79-fw4p-25p8">https://github.com/advisories/GHSA-hg79-fw4p-25p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hg79-fw4p-25p8</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-30T16:40:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[volcano.sh/volcano] Volcano Scheduler Denial of Service via Unbounded Response from Elastic Service/extender Plugin]]></title>
        <id>https://github.com/advisories/GHSA-hg79-fw4p-25p8</id>
        <link href="https://github.com/advisories/GHSA-hg79-fw4p-25p8"/>
        <updated>2025-05-01T13:30:15.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>This issue allows an attacker who has compromised either the Elastic service or the extender plugin to cause denial of service of the scheduler. This is a privilege escalation, because Volcano users may run their Elastic service and extender plugins in separate pods or nodes from the scheduler. In the Kubernetes security model, node isolation is a security boundary, and as such an attacker is able to cross that boundary in Volcano's case if they have compromised either the vulnerable services or the pod/node in which they are deployed.  The scheduler will become unavailable to other users and workloads in the cluster. The scheduler will either crash with an unrecoverable OOM panic or freeze while consuming excessive amounts of memory.</p>
<h3 id="workarounds">Workarounds</h3>
<p>No</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8">https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398">https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d">https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a">https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3">https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4">https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-32777">https://nvd.nist.gov/vuln/detail/CVE-2025-32777</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.10.2">https://github.com/volcano-sh/volcano/releases/tag/v1.10.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3">https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.2">https://github.com/volcano-sh/volcano/releases/tag/v1.11.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2">https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.9.1">https://github.com/volcano-sh/volcano/releases/tag/v1.9.1</a></li>
<li><a href="https://github.com/advisories/GHSA-hg79-fw4p-25p8">https://github.com/advisories/GHSA-hg79-fw4p-25p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hg79-fw4p-25p8</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-30T16:40:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[volcano.sh/volcano] Volcano Scheduler Denial of Service via Unbounded Response from Elastic Service/extender Plugin]]></title>
        <id>https://github.com/advisories/GHSA-hg79-fw4p-25p8</id>
        <link href="https://github.com/advisories/GHSA-hg79-fw4p-25p8"/>
        <updated>2025-05-01T13:30:15.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>This issue allows an attacker who has compromised either the Elastic service or the extender plugin to cause denial of service of the scheduler. This is a privilege escalation, because Volcano users may run their Elastic service and extender plugins in separate pods or nodes from the scheduler. In the Kubernetes security model, node isolation is a security boundary, and as such an attacker is able to cross that boundary in Volcano's case if they have compromised either the vulnerable services or the pod/node in which they are deployed.  The scheduler will become unavailable to other users and workloads in the cluster. The scheduler will either crash with an unrecoverable OOM panic or freeze while consuming excessive amounts of memory.</p>
<h3 id="workarounds">Workarounds</h3>
<p>No</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8">https://github.com/volcano-sh/volcano/security/advisories/GHSA-hg79-fw4p-25p8</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398">https://github.com/volcano-sh/volcano/commit/45a4347471a5254121d10afef04c6732095fa398</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d">https://github.com/volcano-sh/volcano/commit/7103c18de19821cd278f949fa24c13da350a8c5d</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a">https://github.com/volcano-sh/volcano/commit/735842af59b9be0da5090677db7693c98a798b2a</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3">https://github.com/volcano-sh/volcano/commit/7c0ea53fa3cfa7a05b5fba7a8af7bfe88adc41c3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4">https://github.com/volcano-sh/volcano/commit/d687f75a11fa36f37b54e4b6ff8e49bc0a3ca6b4</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-32777">https://nvd.nist.gov/vuln/detail/CVE-2025-32777</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.10.2">https://github.com/volcano-sh/volcano/releases/tag/v1.10.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3">https://github.com/volcano-sh/volcano/releases/tag/v1.11.0-network-topology-preview.3</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.11.2">https://github.com/volcano-sh/volcano/releases/tag/v1.11.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2">https://github.com/volcano-sh/volcano/releases/tag/v1.12.0-alpha.2</a></li>
<li><a href="https://github.com/volcano-sh/volcano/releases/tag/v1.9.1">https://github.com/volcano-sh/volcano/releases/tag/v1.9.1</a></li>
<li><a href="https://github.com/advisories/GHSA-hg79-fw4p-25p8">https://github.com/advisories/GHSA-hg79-fw4p-25p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-hg79-fw4p-25p8</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-30T16:40:03.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/kyverno/kyverno] Kyverno vulnerable to bypass of policy rules that use namespace selectors in match statements]]></title>
        <id>https://github.com/advisories/GHSA-jrr2-x33p-6hvc</id>
        <link href="https://github.com/advisories/GHSA-jrr2-x33p-6hvc"/>
        <updated>2025-04-30T17:29:39.000Z</updated>
        <content type="html"><![CDATA[<h3 id="summary">Summary</h3>
<p>Due to a missing error propagation in function <code>GetNamespaceSelectorsFromNamespaceLister</code> in <code>pkg/utils/engine/labels.go</code> it may happen that policy rules using namespace selector(s) in their <code>match</code> statements are mistakenly not applied during admission review request processing. As a consequence, security-critical mutations and validations are bypassed, potentially allowing attackers with K8s API access to perform malicious operations.</p>
<h3 id="details">Details</h3>
<p>As a policy engine Kyverno is a critical component ensuring the security of Kubernetes clusters by apply security-relevant policy rules in the Kubernetes admission control process.</p>
<p>We encountered a case where Kyverno did not apply policy rules which should have been applied.  This happened in both the mutation and the validation phase of admission control.  Effectively Kyverno handled the admission review requests as
if those policy rules did not exist.  Consequently, the Kube API request was accepted without applying security-relevant patches and validations.</p>
<p>As the root cause we identified a missing error propagation in function <code>GetNamespaceSelectorsFromNamespaceLister</code> in <code>pkg/utils/engine/labels.go</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>).</p>
<p>All affected policy rules use a namespace selector in their match resource filters like this:</p>
<pre><code class="language-yaml">match:
  all:
  - resources:
      namespaceSelector:
        matchExpressions:
        - key: label1
          operator: Exists
</code></pre>
<p>Such specification intents to apply rules only to resource objects which reside in a namespace whose labels match the given label expressions.</p>
<p>When Kyverno handles an admission webhook, function <code>GetNamespaceSelectorsFromNamespaceLister</code> in package
<code>github.com/kyverno/kyverno/pkg/utils/engine</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>) is called to retrieve the labels of the request object's namespace.  This function gets the namespace object from a <code>"k8s.io/client-go/listers/core/v1".NamespaceLister</code>.  In case the
namespace lister returns an error, <code>GetNamespaceSelectorsFromNamespaceLister</code> does NOT propagate this error to its caller, but returns an empty label map, which is equivalent to a namespace without any labels.</p>
<p>The returned label map is later used to select matching policy rules.  If a rule has a resource filter with namespace selector, it will be mistakenly excluded or included.</p>
<p>The namespace lister fails to return the namespace object if the underlying <code>SharedIndexInformer</code> has not (yet) updated its cache.  Those updates happen based on watch events from the Kube API Server, which does not guarantee any maximum delivery time.  If the Kube API Server handling the watch is under high load or otherwise impaired (e.g. requests to etcd take longer due to pending leader election in HA setup) then informer cache updates can be delayed significantly.  However, we did not find a way to reliably reproduce such condition.</p>
<p>To bypass Kyverno policies, an attacker may try to exploit the described misbehavior by:</p>
<ul>
<li><p>putting the Kube API Server under load before sending requests that Kyverno policies should be bypassed for.</p>
</li>
<li><p>sending many request with a high rate to Kube API Server.</p>
</li>
</ul>
<p>We did not try any of such attack vectors and therefore cannot prove their effectiveness.</p>
<p>In our scenario the Kyverno policies apply to pods in "sandbox" namespaces identified as such by certain labels.  Those single-use namespaces and the pods therein are frequently created (and removed) by other controllers.  Therefore, Kyverno often receives admission webhooks for objects whose namespace has been created shortly before.</p>
<h4 id="correction-proposal">Correction Proposal</h4>
<p>Function <code>GetNamespaceSelectorsFromNamespaceLister</code> in package <code>github.com/kyverno/kyverno/pkg/utils/engine</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>) should return an error instead of an empty label map in case it could not get the namespace object from the namespace lister.  This error will then cause admission webhook processing to fail, which lets Kubernetes fail the Kube API request if the policy's failure policy is <code>Fail</code> (a must for security-relevant policies).</p>
<p>In addition, function <code>GetNamespaceSelectorsFromNamespaceLister</code> could retry (with deadline) to get the namespace object from the namespace lister in case of a NotFound error.  But as admission webhook processing time should be kept as short as possible, this might not be a good idea.</p>
<p>Another option would be to perform a GET request for the namespace as a fallback in case the namespace lister returns a NotFound error.</p>
<h3 id="poc">PoC</h3>
<p>We did not find a way to reliably reproduce such case.</p>
<h3 id="impact">Impact</h3>
<p>Administrators attempting to enforce cluster security through Kyverno policies, but that allow less privileged users or service accounts to create/update/delete resources.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/kyverno/kyverno/security/advisories/GHSA-jrr2-x33p-6hvc">https://github.com/kyverno/kyverno/security/advisories/GHSA-jrr2-x33p-6hvc</a></li>
<li><a href="https://github.com/kyverno/kyverno/commit/3ff923b7756e1681daf73849954bd88516589194">https://github.com/kyverno/kyverno/commit/3ff923b7756e1681daf73849954bd88516589194</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-46342">https://nvd.nist.gov/vuln/detail/CVE-2025-46342</a></li>
<li><a href="https://github.com/advisories/GHSA-jrr2-x33p-6hvc">https://github.com/advisories/GHSA-jrr2-x33p-6hvc</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-jrr2-x33p-6hvc</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-29T16:39:33.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/kyverno/kyverno] Kyverno vulnerable to bypass of policy rules that use namespace selectors in match statements]]></title>
        <id>https://github.com/advisories/GHSA-jrr2-x33p-6hvc</id>
        <link href="https://github.com/advisories/GHSA-jrr2-x33p-6hvc"/>
        <updated>2025-04-30T17:29:39.000Z</updated>
        <content type="html"><![CDATA[<h3 id="summary">Summary</h3>
<p>Due to a missing error propagation in function <code>GetNamespaceSelectorsFromNamespaceLister</code> in <code>pkg/utils/engine/labels.go</code> it may happen that policy rules using namespace selector(s) in their <code>match</code> statements are mistakenly not applied during admission review request processing. As a consequence, security-critical mutations and validations are bypassed, potentially allowing attackers with K8s API access to perform malicious operations.</p>
<h3 id="details">Details</h3>
<p>As a policy engine Kyverno is a critical component ensuring the security of Kubernetes clusters by apply security-relevant policy rules in the Kubernetes admission control process.</p>
<p>We encountered a case where Kyverno did not apply policy rules which should have been applied.  This happened in both the mutation and the validation phase of admission control.  Effectively Kyverno handled the admission review requests as
if those policy rules did not exist.  Consequently, the Kube API request was accepted without applying security-relevant patches and validations.</p>
<p>As the root cause we identified a missing error propagation in function <code>GetNamespaceSelectorsFromNamespaceLister</code> in <code>pkg/utils/engine/labels.go</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>).</p>
<p>All affected policy rules use a namespace selector in their match resource filters like this:</p>
<pre><code class="language-yaml">match:
  all:
  - resources:
      namespaceSelector:
        matchExpressions:
        - key: label1
          operator: Exists
</code></pre>
<p>Such specification intents to apply rules only to resource objects which reside in a namespace whose labels match the given label expressions.</p>
<p>When Kyverno handles an admission webhook, function <code>GetNamespaceSelectorsFromNamespaceLister</code> in package
<code>github.com/kyverno/kyverno/pkg/utils/engine</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>) is called to retrieve the labels of the request object's namespace.  This function gets the namespace object from a <code>"k8s.io/client-go/listers/core/v1".NamespaceLister</code>.  In case the
namespace lister returns an error, <code>GetNamespaceSelectorsFromNamespaceLister</code> does NOT propagate this error to its caller, but returns an empty label map, which is equivalent to a namespace without any labels.</p>
<p>The returned label map is later used to select matching policy rules.  If a rule has a resource filter with namespace selector, it will be mistakenly excluded or included.</p>
<p>The namespace lister fails to return the namespace object if the underlying <code>SharedIndexInformer</code> has not (yet) updated its cache.  Those updates happen based on watch events from the Kube API Server, which does not guarantee any maximum delivery time.  If the Kube API Server handling the watch is under high load or otherwise impaired (e.g. requests to etcd take longer due to pending leader election in HA setup) then informer cache updates can be delayed significantly.  However, we did not find a way to reliably reproduce such condition.</p>
<p>To bypass Kyverno policies, an attacker may try to exploit the described misbehavior by:</p>
<ul>
<li><p>putting the Kube API Server under load before sending requests that Kyverno policies should be bypassed for.</p>
</li>
<li><p>sending many request with a high rate to Kube API Server.</p>
</li>
</ul>
<p>We did not try any of such attack vectors and therefore cannot prove their effectiveness.</p>
<p>In our scenario the Kyverno policies apply to pods in "sandbox" namespaces identified as such by certain labels.  Those single-use namespaces and the pods therein are frequently created (and removed) by other controllers.  Therefore, Kyverno often receives admission webhooks for objects whose namespace has been created shortly before.</p>
<h4 id="correction-proposal">Correction Proposal</h4>
<p>Function <code>GetNamespaceSelectorsFromNamespaceLister</code> in package <code>github.com/kyverno/kyverno/pkg/utils/engine</code> (<a href="https://github.com/kyverno/kyverno/blob/a96b1a4794b4d25cb0c6d72c05fc6355e95cf65c/pkg/utils/engine/labels.go#L10">src</a>) should return an error instead of an empty label map in case it could not get the namespace object from the namespace lister.  This error will then cause admission webhook processing to fail, which lets Kubernetes fail the Kube API request if the policy's failure policy is <code>Fail</code> (a must for security-relevant policies).</p>
<p>In addition, function <code>GetNamespaceSelectorsFromNamespaceLister</code> could retry (with deadline) to get the namespace object from the namespace lister in case of a NotFound error.  But as admission webhook processing time should be kept as short as possible, this might not be a good idea.</p>
<p>Another option would be to perform a GET request for the namespace as a fallback in case the namespace lister returns a NotFound error.</p>
<h3 id="poc">PoC</h3>
<p>We did not find a way to reliably reproduce such case.</p>
<h3 id="impact">Impact</h3>
<p>Administrators attempting to enforce cluster security through Kyverno policies, but that allow less privileged users or service accounts to create/update/delete resources.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/kyverno/kyverno/security/advisories/GHSA-jrr2-x33p-6hvc">https://github.com/kyverno/kyverno/security/advisories/GHSA-jrr2-x33p-6hvc</a></li>
<li><a href="https://github.com/kyverno/kyverno/commit/3ff923b7756e1681daf73849954bd88516589194">https://github.com/kyverno/kyverno/commit/3ff923b7756e1681daf73849954bd88516589194</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-46342">https://nvd.nist.gov/vuln/detail/CVE-2025-46342</a></li>
<li><a href="https://github.com/advisories/GHSA-jrr2-x33p-6hvc">https://github.com/advisories/GHSA-jrr2-x33p-6hvc</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-jrr2-x33p-6hvc</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-29T16:39:33.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/snowflakedb/gosnowflake] Go Snowflake Driver has race condition when checking access to Easy Logging configuration file]]></title>
        <id>https://github.com/advisories/GHSA-6jgm-j7h2-2fqg</id>
        <link href="https://github.com/advisories/GHSA-6jgm-j7h2-2fqg"/>
        <updated>2025-04-29T13:10:42.000Z</updated>
        <content type="html"><![CDATA[<h1 id="issue">Issue</h1>
<p>Snowflake discovered and remediated a vulnerability in the Go Snowflake Driver (“Driver”). When using the Easy Logging feature on Linux and macOS, the Driver didn’t correctly verify the permissions of the logging configuration file, potentially allowing an attacker with local access to overwrite the configuration and gain control over logging level and output location.</p>
<p>This vulnerability affects Driver versions from 1.7.0 up to, but not including, 1.13.3. Snowflake fixed the issue in version 1.13.3.</p>
<h1 id="vulnerability-details">Vulnerability Details</h1>
<p>When using the Easy Logging feature on Linux and macOS, the Driver reads logging configuration from a user-provided file. On Linux and macOS the Driver verifies that the configuration file can be written to only by its owner. That check was vulnerable to a Time-of-Check to Time-of-Use (TOCTOU) race condition and failed to verify that the file owner matches the user running the Driver. This could allow a local attacker with write access to the configuration file or the directory containing it to overwrite the configuration and gain control over logging level and output location.</p>
<h1 id="solution">Solution</h1>
<p>Snowflake released version 1.13.3 of the Go Snowflake Driver, which fixes this issue. We recommend users upgrade to version 1.13.3.</p>
<h1 id="additional-information">Additional Information</h1>
<p>If you discover a security vulnerability in one of our products or websites, please report the issue to Snowflake through our Vulnerability Disclosure Program hosted at HackerOne. For more information, please see our <a href="https://hackerone.com/snowflake?type=team">Vulnerability Disclosure Policy</a>.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://github.com/snowflakedb/gosnowflake/security/advisories/GHSA-6jgm-j7h2-2fqg">https://github.com/snowflakedb/gosnowflake/security/advisories/GHSA-6jgm-j7h2-2fqg</a></li>
<li><a href="https://github.com/snowflakedb/gosnowflake/commit/ba94a4800e23621eff558ef18ce4b96ec5489ff0">https://github.com/snowflakedb/gosnowflake/commit/ba94a4800e23621eff558ef18ce4b96ec5489ff0</a></li>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-46327">https://nvd.nist.gov/vuln/detail/CVE-2025-46327</a></li>
<li><a href="https://github.com/advisories/GHSA-6jgm-j7h2-2fqg">https://github.com/advisories/GHSA-6jgm-j7h2-2fqg</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-6jgm-j7h2-2fqg</uri>
        </author>
        <category label="severity" term="LOW"/>
        <published>2025-04-28T20:27:29.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/steve] Steve doesn’t verify a server’s certificate and is susceptible to man-in-the-middle (MitM) attacks]]></title>
        <id>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</id>
        <link href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx"/>
        <updated>2025-04-25T15:12:45.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified in Steve where by default it was using an insecure option that did not validate the certificate presented by the remote server while performing a TLS connection. This could allow the execution of a man-in-the-middle (MitM) attack against services using Steve.</p>
<p>For example, Rancher relies on Steve as a dependency for its user interface (UI) to proxy requests to Kubernetes clusters. Users who have the permission to create a service in Rancher’s local cluster can take over Rancher’s UI and display their own UI to gather sensitive information. This is only possible when the setting <code>ui-offline-preferred</code> is manually set to <code>remote</code> (by default Rancher sets it to <code>dynamic</code>). This enables further attacks such as cross-site scripting (XSS), or tampering the UI to collect passwords from other users etc.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions of Steve include releases <code>v0.2.1</code>, <code>v0.3.3</code>, <code>v0.4.4</code> and <code>v0.5.13</code>.</p>
<p>This vulnerability is addressed by changing Steve to always verify a server’s certificate based on Go’s TLS settings.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that you are only using Steve to connect to trusted servers.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx</a></li>
<li><a href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/advisories/GHSA-95fc-g4gj-mqmx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:12:44.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/steve] Steve doesn’t verify a server’s certificate and is susceptible to man-in-the-middle (MitM) attacks]]></title>
        <id>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</id>
        <link href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx"/>
        <updated>2025-04-25T15:12:45.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified in Steve where by default it was using an insecure option that did not validate the certificate presented by the remote server while performing a TLS connection. This could allow the execution of a man-in-the-middle (MitM) attack against services using Steve.</p>
<p>For example, Rancher relies on Steve as a dependency for its user interface (UI) to proxy requests to Kubernetes clusters. Users who have the permission to create a service in Rancher’s local cluster can take over Rancher’s UI and display their own UI to gather sensitive information. This is only possible when the setting <code>ui-offline-preferred</code> is manually set to <code>remote</code> (by default Rancher sets it to <code>dynamic</code>). This enables further attacks such as cross-site scripting (XSS), or tampering the UI to collect passwords from other users etc.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions of Steve include releases <code>v0.2.1</code>, <code>v0.3.3</code>, <code>v0.4.4</code> and <code>v0.5.13</code>.</p>
<p>This vulnerability is addressed by changing Steve to always verify a server’s certificate based on Go’s TLS settings.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that you are only using Steve to connect to trusted servers.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx</a></li>
<li><a href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/advisories/GHSA-95fc-g4gj-mqmx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:12:44.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/stev] Steve doesn’t verify a server’s certificate and is susceptible to man-in-the-middle (MitM) attacks]]></title>
        <id>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</id>
        <link href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx"/>
        <updated>2025-04-25T15:12:45.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified in Steve where by default it was using an insecure option that did not validate the certificate presented by the remote server while performing a TLS connection. This could allow the execution of a man-in-the-middle (MitM) attack against services using Steve.</p>
<p>For example, Rancher relies on Steve as a dependency for its user interface (UI) to proxy requests to Kubernetes clusters. Users who have the permission to create a service in Rancher’s local cluster can take over Rancher’s UI and display their own UI to gather sensitive information. This is only possible when the setting <code>ui-offline-preferred</code> is manually set to <code>remote</code> (by default Rancher sets it to <code>dynamic</code>). This enables further attacks such as cross-site scripting (XSS), or tampering the UI to collect passwords from other users etc.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions of Steve include releases <code>v0.2.1</code>, <code>v0.3.3</code>, <code>v0.4.4</code> and <code>v0.5.13</code>.</p>
<p>This vulnerability is addressed by changing Steve to always verify a server’s certificate based on Go’s TLS settings.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that you are only using Steve to connect to trusted servers.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx</a></li>
<li><a href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/advisories/GHSA-95fc-g4gj-mqmx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:12:44.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/steve] Steve doesn’t verify a server’s certificate and is susceptible to man-in-the-middle (MitM) attacks]]></title>
        <id>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</id>
        <link href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx"/>
        <updated>2025-04-25T15:12:45.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified in Steve where by default it was using an insecure option that did not validate the certificate presented by the remote server while performing a TLS connection. This could allow the execution of a man-in-the-middle (MitM) attack against services using Steve.</p>
<p>For example, Rancher relies on Steve as a dependency for its user interface (UI) to proxy requests to Kubernetes clusters. Users who have the permission to create a service in Rancher’s local cluster can take over Rancher’s UI and display their own UI to gather sensitive information. This is only possible when the setting <code>ui-offline-preferred</code> is manually set to <code>remote</code> (by default Rancher sets it to <code>dynamic</code>). This enables further attacks such as cross-site scripting (XSS), or tampering the UI to collect passwords from other users etc.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions of Steve include releases <code>v0.2.1</code>, <code>v0.3.3</code>, <code>v0.4.4</code> and <code>v0.5.13</code>.</p>
<p>This vulnerability is addressed by changing Steve to always verify a server’s certificate based on Go’s TLS settings.</p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that you are only using Steve to connect to trusted servers.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/rancher/steve/security/advisories/GHSA-95fc-g4gj-mqmx</a></li>
<li><a href="https://github.com/advisories/GHSA-95fc-g4gj-mqmx">https://github.com/advisories/GHSA-95fc-g4gj-mqmx</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-95fc-g4gj-mqmx</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:12:44.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/fleet] Fleet doesn’t validate a server’s certificate when connecting through SSH]]></title>
        <id>https://github.com/advisories/GHSA-xgpc-q899-67p8</id>
        <link href="https://github.com/advisories/GHSA-xgpc-q899-67p8"/>
        <updated>2025-04-25T15:11:08.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Fleet where, by default, Fleet will automatically trust a remote server’s certificate when connecting through SSH if the certificate isn’t set in the <code>known_hosts</code> file. This could allow the execution of a man-in-the-middle (MitM) attack against Fleet. In case the server that is being connected to has a trusted entry in the known_hosts file, then Fleet will correctly check the authenticity of the presented certificate. </p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v0.10.12</code>, <code>v0.11.7</code> and <code>v0.12.2</code>.</p>
<p>The fix involves some key areas with the following changes:</p>
<ul>
<li><p>Git latest commit fetcher sources <code>known_hosts</code> entries from the following locations, in decreasing order of priority:</p>
<ol>
<li>Secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field;</li>
<li>If no secret is referenced, in a <code>gitcredential</code> secret located in the <code>GitRepo</code>’s namespace;</li>
<li>If that secret does not exist, in a (new) <code>known-hosts</code> config map installed by Fleet, populated statically with public entries shared by a few git providers: Github, Gitlab, Bitbucket, Azure DevOps;</li>
</ol>
</li>
<li><p>Git cloner: same as above.</p>
</li>
<li><p><code>fleet apply</code> command: same as above. The command reads entries from a <code>FLEET_KNOWN_HOSTS</code> environment variable. That command is typically run within a container inside a job pod created by Fleet to update bundles from a new commit. However, users may also decide to run it locally, perhaps even with multiple concurrent executions of the command on the same machine. To cater for this, <code>fleet apply</code> writes the contents of <code>FLEET_KNOWN_HOSTS</code>, if any, to a temporary file with a random name, and deletes that file once bundles have been created. This reduces the risk of conflicts between concurrent runs.
This happens regardless of the git repository URL (SSH or not), since a repository may reference artifacts to be retrieved using SSH anyway.</p>
</li>
</ul>
<p><strong>Note about sourcing <code>known_hosts</code> entries:</strong> if entries are found in a supported source, whatever that source may be, then those entries will be used. For instance, if wrong entries, or an incomplete set of entries (e.g. only BitBucket entries for a <code>GitRepo</code> pointing to Github) are found in a secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field, they will still be used. This will lead to errors if strict host key checks are enabled, even if matching, correct entries are found in another source with lower priority, such as the <code>known-hosts</code> config map. Fleet will not use one source to complement the other.</p>
<p><strong>Note: Fleet v0.9 release line does not have the fix for this CVE. The fix for v0.9 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>There are no workarounds for this issue. Users are recommended to upgrade, as soon as possible, to a version of Fleet that contains the fixes.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8">https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3571">https://github.com/rancher/fleet/pull/3571</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3572">https://github.com/rancher/fleet/pull/3572</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3573">https://github.com/rancher/fleet/pull/3573</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.10.12">https://github.com/rancher/fleet/releases/tag/v0.10.12</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.11.7">https://github.com/rancher/fleet/releases/tag/v0.11.7</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.12.2">https://github.com/rancher/fleet/releases/tag/v0.12.2</a></li>
<li><a href="https://github.com/advisories/GHSA-xgpc-q899-67p8">https://github.com/advisories/GHSA-xgpc-q899-67p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-xgpc-q899-67p8</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2025-04-25T15:11:07.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/fleet] Fleet doesn’t validate a server’s certificate when connecting through SSH]]></title>
        <id>https://github.com/advisories/GHSA-xgpc-q899-67p8</id>
        <link href="https://github.com/advisories/GHSA-xgpc-q899-67p8"/>
        <updated>2025-04-25T15:11:08.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Fleet where, by default, Fleet will automatically trust a remote server’s certificate when connecting through SSH if the certificate isn’t set in the <code>known_hosts</code> file. This could allow the execution of a man-in-the-middle (MitM) attack against Fleet. In case the server that is being connected to has a trusted entry in the known_hosts file, then Fleet will correctly check the authenticity of the presented certificate. </p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v0.10.12</code>, <code>v0.11.7</code> and <code>v0.12.2</code>.</p>
<p>The fix involves some key areas with the following changes:</p>
<ul>
<li><p>Git latest commit fetcher sources <code>known_hosts</code> entries from the following locations, in decreasing order of priority:</p>
<ol>
<li>Secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field;</li>
<li>If no secret is referenced, in a <code>gitcredential</code> secret located in the <code>GitRepo</code>’s namespace;</li>
<li>If that secret does not exist, in a (new) <code>known-hosts</code> config map installed by Fleet, populated statically with public entries shared by a few git providers: Github, Gitlab, Bitbucket, Azure DevOps;</li>
</ol>
</li>
<li><p>Git cloner: same as above.</p>
</li>
<li><p><code>fleet apply</code> command: same as above. The command reads entries from a <code>FLEET_KNOWN_HOSTS</code> environment variable. That command is typically run within a container inside a job pod created by Fleet to update bundles from a new commit. However, users may also decide to run it locally, perhaps even with multiple concurrent executions of the command on the same machine. To cater for this, <code>fleet apply</code> writes the contents of <code>FLEET_KNOWN_HOSTS</code>, if any, to a temporary file with a random name, and deletes that file once bundles have been created. This reduces the risk of conflicts between concurrent runs.
This happens regardless of the git repository URL (SSH or not), since a repository may reference artifacts to be retrieved using SSH anyway.</p>
</li>
</ul>
<p><strong>Note about sourcing <code>known_hosts</code> entries:</strong> if entries are found in a supported source, whatever that source may be, then those entries will be used. For instance, if wrong entries, or an incomplete set of entries (e.g. only BitBucket entries for a <code>GitRepo</code> pointing to Github) are found in a secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field, they will still be used. This will lead to errors if strict host key checks are enabled, even if matching, correct entries are found in another source with lower priority, such as the <code>known-hosts</code> config map. Fleet will not use one source to complement the other.</p>
<p><strong>Note: Fleet v0.9 release line does not have the fix for this CVE. The fix for v0.9 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>There are no workarounds for this issue. Users are recommended to upgrade, as soon as possible, to a version of Fleet that contains the fixes.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8">https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3571">https://github.com/rancher/fleet/pull/3571</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3572">https://github.com/rancher/fleet/pull/3572</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3573">https://github.com/rancher/fleet/pull/3573</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.10.12">https://github.com/rancher/fleet/releases/tag/v0.10.12</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.11.7">https://github.com/rancher/fleet/releases/tag/v0.11.7</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.12.2">https://github.com/rancher/fleet/releases/tag/v0.12.2</a></li>
<li><a href="https://github.com/advisories/GHSA-xgpc-q899-67p8">https://github.com/advisories/GHSA-xgpc-q899-67p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-xgpc-q899-67p8</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2025-04-25T15:11:07.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/fleet] Fleet doesn’t validate a server’s certificate when connecting through SSH]]></title>
        <id>https://github.com/advisories/GHSA-xgpc-q899-67p8</id>
        <link href="https://github.com/advisories/GHSA-xgpc-q899-67p8"/>
        <updated>2025-04-25T15:11:08.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Fleet where, by default, Fleet will automatically trust a remote server’s certificate when connecting through SSH if the certificate isn’t set in the <code>known_hosts</code> file. This could allow the execution of a man-in-the-middle (MitM) attack against Fleet. In case the server that is being connected to has a trusted entry in the known_hosts file, then Fleet will correctly check the authenticity of the presented certificate. </p>
<p>Please consult the associated  <a href="https://attack.mitre.org/techniques/T1557/">MITRE ATT&amp;CK - Technique - Adversary-in-the-Middle</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v0.10.12</code>, <code>v0.11.7</code> and <code>v0.12.2</code>.</p>
<p>The fix involves some key areas with the following changes:</p>
<ul>
<li><p>Git latest commit fetcher sources <code>known_hosts</code> entries from the following locations, in decreasing order of priority:</p>
<ol>
<li>Secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field;</li>
<li>If no secret is referenced, in a <code>gitcredential</code> secret located in the <code>GitRepo</code>’s namespace;</li>
<li>If that secret does not exist, in a (new) <code>known-hosts</code> config map installed by Fleet, populated statically with public entries shared by a few git providers: Github, Gitlab, Bitbucket, Azure DevOps;</li>
</ol>
</li>
<li><p>Git cloner: same as above.</p>
</li>
<li><p><code>fleet apply</code> command: same as above. The command reads entries from a <code>FLEET_KNOWN_HOSTS</code> environment variable. That command is typically run within a container inside a job pod created by Fleet to update bundles from a new commit. However, users may also decide to run it locally, perhaps even with multiple concurrent executions of the command on the same machine. To cater for this, <code>fleet apply</code> writes the contents of <code>FLEET_KNOWN_HOSTS</code>, if any, to a temporary file with a random name, and deletes that file once bundles have been created. This reduces the risk of conflicts between concurrent runs.
This happens regardless of the git repository URL (SSH or not), since a repository may reference artifacts to be retrieved using SSH anyway.</p>
</li>
</ul>
<p><strong>Note about sourcing <code>known_hosts</code> entries:</strong> if entries are found in a supported source, whatever that source may be, then those entries will be used. For instance, if wrong entries, or an incomplete set of entries (e.g. only BitBucket entries for a <code>GitRepo</code> pointing to Github) are found in a secret referenced in a <code>GitRepo</code>’s <code>clientSecretName</code> field, they will still be used. This will lead to errors if strict host key checks are enabled, even if matching, correct entries are found in another source with lower priority, such as the <code>known-hosts</code> config map. Fleet will not use one source to complement the other.</p>
<p><strong>Note: Fleet v0.9 release line does not have the fix for this CVE. The fix for v0.9 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>There are no workarounds for this issue. Users are recommended to upgrade, as soon as possible, to a version of Fleet that contains the fixes.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8">https://github.com/rancher/fleet/security/advisories/GHSA-xgpc-q899-67p8</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3571">https://github.com/rancher/fleet/pull/3571</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3572">https://github.com/rancher/fleet/pull/3572</a></li>
<li><a href="https://github.com/rancher/fleet/pull/3573">https://github.com/rancher/fleet/pull/3573</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.10.12">https://github.com/rancher/fleet/releases/tag/v0.10.12</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.11.7">https://github.com/rancher/fleet/releases/tag/v0.11.7</a></li>
<li><a href="https://github.com/rancher/fleet/releases/tag/v0.12.2">https://github.com/rancher/fleet/releases/tag/v0.12.2</a></li>
<li><a href="https://github.com/advisories/GHSA-xgpc-q899-67p8">https://github.com/advisories/GHSA-xgpc-q899-67p8</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-xgpc-q899-67p8</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2025-04-25T15:11:07.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/rancher] Rancher users who can create Projects can gain access to arbitrary projects]]></title>
        <id>https://github.com/advisories/GHSA-8h6m-wv39-239m</id>
        <link href="https://github.com/advisories/GHSA-8h6m-wv39-239m"/>
        <updated>2025-04-25T15:09:27.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Rancher where a user with the ability to create a project, on a certain cluster, can create a project with the same name as an existing project in a different cluster. This results in the user gaining access to the other project in the different cluster, resulting in a privilege escalation. This happens because the namespace used on the local cluster to store related resources (PRTBs and secrets) is the name of the project.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/tactics/TA0004/">MITRE ATT&amp;CK - Technique - Privilege Escalation</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v2.11.1</code>, <code>v2.10.5</code>, <code>v2.9.9</code>.</p>
<p>The fix involves the following changes:</p>
<p><strong>Rancher:</strong></p>
<ul>
<li>Instead of using the project name as the namespace, Rancher will instead be using a new field on the project spec called backingNamespace. If that field exists, use that for the project namespace going forward. However, if the project does not have that field filled out (likely because it existed before this change), Rancher will continue using the name for the namespace.</li>
</ul>
<p><strong>Rancher Webhook:</strong></p>
<ul>
<li>New mutation on create <code>project.Status.BackingNamespace</code> to be <code>SafeConcatName(project.Spec.ClusterName, project.Name)</code>;</li>
<li>Generate the name manually within the mutating webhook, because normally, name generation happens after the mutating webhooks;</li>
<li>Removed a validation where <code>projectName</code> and <code>Namespace</code> had to be the same for PRTBs, since PRTBs now go in <code>project.BackingNamespace</code>;</li>
<li>On update, if <code>BackingNamespace</code> isn't set, set it to <code>project.Name</code>. For existing objects after update this will help unify them to the new projects.</li>
<li>The <code>BackingNamespace</code> can't be edited after it's set.</li>
</ul>
<p><strong>Note: Rancher v2.8 release line does not have the fix for this CVE. The fix for v2.8 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that:</p>
<ul>
<li>Users are not allowed to create projects with the same object names from another cluster.</li>
</ul>
<p>To identify if this security issue could have been abused within your system, you need to find if there are any projects with the same name but on different clusters. To do that, run the following command in the local cluster as an administrator:</p>
<pre><code>kubectl get projects -A -o=custom-columns='NAME:metadata.name' | sort | uniq -c
</code></pre>
<p>That command will list all project names, and show the instances of each name. Any project with more than 1 instance is affected by this security issue. To remedy the situation, the projects will need to be deleted and re-created to ensure no namespace collisions happen. While it would be possible to delete all but 1 of the projects with the same name, this is unadvisable because a user could have given themselves access to the wrong project.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m">https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m</a></li>
<li><a href="https://github.com/advisories/GHSA-8h6m-wv39-239m">https://github.com/advisories/GHSA-8h6m-wv39-239m</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-8h6m-wv39-239m</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:09:26.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/rancher] Rancher users who can create Projects can gain access to arbitrary projects]]></title>
        <id>https://github.com/advisories/GHSA-8h6m-wv39-239m</id>
        <link href="https://github.com/advisories/GHSA-8h6m-wv39-239m"/>
        <updated>2025-04-25T15:09:27.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Rancher where a user with the ability to create a project, on a certain cluster, can create a project with the same name as an existing project in a different cluster. This results in the user gaining access to the other project in the different cluster, resulting in a privilege escalation. This happens because the namespace used on the local cluster to store related resources (PRTBs and secrets) is the name of the project.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/tactics/TA0004/">MITRE ATT&amp;CK - Technique - Privilege Escalation</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v2.11.1</code>, <code>v2.10.5</code>, <code>v2.9.9</code>.</p>
<p>The fix involves the following changes:</p>
<p><strong>Rancher:</strong></p>
<ul>
<li>Instead of using the project name as the namespace, Rancher will instead be using a new field on the project spec called backingNamespace. If that field exists, use that for the project namespace going forward. However, if the project does not have that field filled out (likely because it existed before this change), Rancher will continue using the name for the namespace.</li>
</ul>
<p><strong>Rancher Webhook:</strong></p>
<ul>
<li>New mutation on create <code>project.Status.BackingNamespace</code> to be <code>SafeConcatName(project.Spec.ClusterName, project.Name)</code>;</li>
<li>Generate the name manually within the mutating webhook, because normally, name generation happens after the mutating webhooks;</li>
<li>Removed a validation where <code>projectName</code> and <code>Namespace</code> had to be the same for PRTBs, since PRTBs now go in <code>project.BackingNamespace</code>;</li>
<li>On update, if <code>BackingNamespace</code> isn't set, set it to <code>project.Name</code>. For existing objects after update this will help unify them to the new projects.</li>
<li>The <code>BackingNamespace</code> can't be edited after it's set.</li>
</ul>
<p><strong>Note: Rancher v2.8 release line does not have the fix for this CVE. The fix for v2.8 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that:</p>
<ul>
<li>Users are not allowed to create projects with the same object names from another cluster.</li>
</ul>
<p>To identify if this security issue could have been abused within your system, you need to find if there are any projects with the same name but on different clusters. To do that, run the following command in the local cluster as an administrator:</p>
<pre><code>kubectl get projects -A -o=custom-columns='NAME:metadata.name' | sort | uniq -c
</code></pre>
<p>That command will list all project names, and show the instances of each name. Any project with more than 1 instance is affected by this security issue. To remedy the situation, the projects will need to be deleted and re-created to ensure no namespace collisions happen. While it would be possible to delete all but 1 of the projects with the same name, this is unadvisable because a user could have given themselves access to the wrong project.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m">https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m</a></li>
<li><a href="https://github.com/advisories/GHSA-8h6m-wv39-239m">https://github.com/advisories/GHSA-8h6m-wv39-239m</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-8h6m-wv39-239m</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:09:26.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/rancher/rancher] Rancher users who can create Projects can gain access to arbitrary projects]]></title>
        <id>https://github.com/advisories/GHSA-8h6m-wv39-239m</id>
        <link href="https://github.com/advisories/GHSA-8h6m-wv39-239m"/>
        <updated>2025-04-25T15:09:27.000Z</updated>
        <content type="html"><![CDATA[<h3 id="impact">Impact</h3>
<p>A vulnerability has been identified within Rancher where a user with the ability to create a project, on a certain cluster, can create a project with the same name as an existing project in a different cluster. This results in the user gaining access to the other project in the different cluster, resulting in a privilege escalation. This happens because the namespace used on the local cluster to store related resources (PRTBs and secrets) is the name of the project.</p>
<p>Please consult the associated  <a href="https://attack.mitre.org/tactics/TA0004/">MITRE ATT&amp;CK - Technique - Privilege Escalation</a> for further information about this category of attack.</p>
<h3 id="patches">Patches</h3>
<p>Patched versions include releases <code>v2.11.1</code>, <code>v2.10.5</code>, <code>v2.9.9</code>.</p>
<p>The fix involves the following changes:</p>
<p><strong>Rancher:</strong></p>
<ul>
<li>Instead of using the project name as the namespace, Rancher will instead be using a new field on the project spec called backingNamespace. If that field exists, use that for the project namespace going forward. However, if the project does not have that field filled out (likely because it existed before this change), Rancher will continue using the name for the namespace.</li>
</ul>
<p><strong>Rancher Webhook:</strong></p>
<ul>
<li>New mutation on create <code>project.Status.BackingNamespace</code> to be <code>SafeConcatName(project.Spec.ClusterName, project.Name)</code>;</li>
<li>Generate the name manually within the mutating webhook, because normally, name generation happens after the mutating webhooks;</li>
<li>Removed a validation where <code>projectName</code> and <code>Namespace</code> had to be the same for PRTBs, since PRTBs now go in <code>project.BackingNamespace</code>;</li>
<li>On update, if <code>BackingNamespace</code> isn't set, set it to <code>project.Name</code>. For existing objects after update this will help unify them to the new projects.</li>
<li>The <code>BackingNamespace</code> can't be edited after it's set.</li>
</ul>
<p><strong>Note: Rancher v2.8 release line does not have the fix for this CVE. The fix for v2.8 was considered too complex and with the risk of introducing instabilities right before this version goes into end-of-life (EOL), as documented in <a href="https://www.suse.com/lifecycle/#suse-rancher-prime">SUSE’s Product Support Lifecycle</a> page. Please see the section below for workarounds or consider upgrading to a newer and patched version of Rancher.</strong></p>
<h3 id="workarounds">Workarounds</h3>
<p>If you can't upgrade to a fixed version, please make sure that:</p>
<ul>
<li>Users are not allowed to create projects with the same object names from another cluster.</li>
</ul>
<p>To identify if this security issue could have been abused within your system, you need to find if there are any projects with the same name but on different clusters. To do that, run the following command in the local cluster as an administrator:</p>
<pre><code>kubectl get projects -A -o=custom-columns='NAME:metadata.name' | sort | uniq -c
</code></pre>
<p>That command will list all project names, and show the instances of each name. Any project with more than 1 instance is affected by this security issue. To remedy the situation, the projects will need to be deleted and re-created to ensure no namespace collisions happen. While it would be possible to delete all but 1 of the projects with the same name, this is unadvisable because a user could have given themselves access to the wrong project.</p>
<h3 id="references">References</h3>
<p>If you have any questions or comments about this advisory:</p>
<ul>
<li>Reach out to the <a href="https://github.com/rancher/rancher/security/policy">SUSE Rancher Security team</a> for security related inquiries.</li>
<li>Open an issue in the <a href="https://github.com/rancher/rancher/issues/new/choose">Rancher</a> repository.</li>
<li>Verify with our <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">support matrix</a> and <a href="https://www.suse.com/lifecycle/">product support lifecycle</a>.</li>
</ul>
<h3 id="references-1">References</h3>
<ul>
<li><a href="https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m">https://github.com/rancher/rancher/security/advisories/GHSA-8h6m-wv39-239m</a></li>
<li><a href="https://github.com/advisories/GHSA-8h6m-wv39-239m">https://github.com/advisories/GHSA-8h6m-wv39-239m</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-8h6m-wv39-239m</uri>
        </author>
        <category label="severity" term="HIGH"/>
        <published>2025-04-25T15:09:26.000Z</published>
    </entry>
    <entry>
        <title type="html"><![CDATA[[github.com/k3s-io/k3s] CNCF K3s Kubernetes kubelet configuration exposes credentials]]></title>
        <id>https://github.com/advisories/GHSA-864f-7xjm-2jp2</id>
        <link href="https://github.com/advisories/GHSA-864f-7xjm-2jp2"/>
        <updated>2025-04-25T15:07:33.000Z</updated>
        <content type="html"><![CDATA[<p>CNCF K3s 1.32 before 1.32.4-rc1+k3s1 has a Kubernetes kubelet configuration change with the unintended consequence that, in some situations, ReadOnlyPort is set to 10255. For example, the default behavior of a K3s online installation might allow unauthenticated access to this port, exposing credentials.</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-46599">https://nvd.nist.gov/vuln/detail/CVE-2025-46599</a></li>
<li><a href="https://github.com/f1veT/BUG/issues/2">https://github.com/f1veT/BUG/issues/2</a></li>
<li><a href="https://github.com/k3s-io/k3s/issues/12164">https://github.com/k3s-io/k3s/issues/12164</a></li>
<li><a href="https://github.com/k3s-io/k3s/commit/097b63e588e3c844cdf9b967bcd0a69f4fc0aa0a">https://github.com/k3s-io/k3s/commit/097b63e588e3c844cdf9b967bcd0a69f4fc0aa0a</a></li>
<li><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/disable-kubelet-readonly-port">https://cloud.google.com/kubernetes-engine/docs/how-to/disable-kubelet-readonly-port</a></li>
<li><a href="https://github.com/k3s-io/k3s/compare/v1.32.3+k3s1...v1.32.4-rc1+k3s1">https://github.com/k3s-io/k3s/compare/v1.32.3+k3s1...v1.32.4-rc1+k3s1</a></li>
<li><a href="https://github.com/advisories/GHSA-864f-7xjm-2jp2">https://github.com/advisories/GHSA-864f-7xjm-2jp2</a></li>
</ul>
]]></content>
        <author>
            <name>GitHub</name>
            <email>GitHub@noreply.github.com</email>
            <uri>https://github.com/advisories/GHSA-864f-7xjm-2jp2</uri>
        </author>
        <category label="severity" term="MODERATE"/>
        <published>2025-04-25T06:30:56.000Z</published>
    </entry>
</feed>